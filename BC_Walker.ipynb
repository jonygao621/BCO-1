{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation for:\n",
    "\n",
    "- Behavioral Cloning (BC) <a href=\"https://arxiv.org/abs/1608.00627v1\">paper link</a>\n",
    "\n",
    "## Author: Montaser Mohammedalamen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install gym\n",
    "! pip install box2d-py\n",
    "! pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "from matplotlib import style\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init environment\n",
    "env_name='BipedalWalker-v2'\n",
    "env = gym.make(env_name)\n",
    "action_space_size = env.action_space.shape[0]\n",
    "state_space_size  = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Expert data (states and actions for BC, States only for BCO)\n",
    "expert_states  = torch.tensor(np.load(\"states_expert_walker2d.npy\"), dtype=torch.float)\n",
    "expert_actions = torch.tensor(np.load(\"actions_expert_walker2d.npy\"), dtype=torch.float)\n",
    "print(\"expert_states\", expert_states.shape)\n",
    "print(\"expert_actions\", expert_actions.shape)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_input (states, actions,  n=2, compare=1):\n",
    "    '''\n",
    "    Data preperpation and filtering \n",
    "    Inputs:\n",
    "    states: expert states as tensor\n",
    "    actions: actions states as tensor\n",
    "    n: window size (how many states needed to predict the next action)\n",
    "    compare: for filtering data \n",
    "    return:\n",
    "    output_states: filtered states as tensor \n",
    "    output_actions: filtered actions as tensor \n",
    "    '''\n",
    "    count=0\n",
    "    index= []\n",
    "    ep, t, state_size = states.shape\n",
    "    _, _, action_size = actions.shape\n",
    "    \n",
    "    output_states = torch.zeros((ep*(t-n+1) , state_size*n), dtype = torch.float)\n",
    "    output_actions = torch.zeros((ep*(t-n+1) , action_size), dtype = torch.float)\n",
    "    \n",
    "    for i in range (ep):\n",
    "        for j in range (t-n+1):\n",
    "            if (states[i, j] == -compare*torch.ones(state_size)).all() or (states[i, j+1] == -compare*torch.ones(state_size)).all():\n",
    "                index.append([i,j])\n",
    "            else:\n",
    "                output_states[count] = states[i, j:j+n].view(-1)\n",
    "                output_actions[count] = actions[i,j]\n",
    "                count+=1\n",
    "    output_states= output_states[:count]\n",
    "    output_actions= output_actions[:count]\n",
    "    \n",
    "    return output_states, output_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting number expert trajectories from expert data\n",
    "number_expert_trajectories = 50\n",
    "a= np.random.randint(expert_states.shape[0] - number_expert_trajectories)\n",
    "print(a)\n",
    "expert_state, expert_action = to_input (expert_states[a : a+number_expert_trajectories], expert_actions[a : a+number_expert_trajectories], n=2,  compare=5)\n",
    "print(\"expert_state\", expert_state.shape)\n",
    "print(\"expert_action\", expert_action.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Behavioral Cloning (BC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate expert states and actions, divided into 70% training and 30% testing\n",
    "\n",
    "new_data = np.concatenate((expert_state[:,: state_space_size], expert_action), axis=1)\n",
    "np.random.shuffle(new_data)\n",
    "new_data = torch.tensor(new_data, dtype=torch.float)\n",
    "n_samples = int(new_data.shape[0]*0.7)\n",
    "training_set = new_data[:n_samples]\n",
    "testing_set = new_data[n_samples:]\n",
    "print(\"training_set\", training_set.shape)\n",
    "print(\"testing_set\", testing_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network arch Behavioral Cloning , loss function and optimizer\n",
    "bc_walker =  nn.Sequential(\n",
    "    nn.Linear(state_space_size,40),\n",
    "    nn.ReLU(),\n",
    "    \n",
    "    nn.Linear(40,80),\n",
    "    nn.ReLU(),\n",
    "    \n",
    "    nn.Linear(80,120),\n",
    "    nn.ReLU(),\n",
    "    \n",
    "    nn.Linear(120,100),\n",
    "    nn.ReLU(),\n",
    "    \n",
    "    nn.Linear(100,40),\n",
    "    nn.ReLU(),\n",
    "    \n",
    "    nn.Linear(40,20),\n",
    "    nn.ReLU(),\n",
    "    \n",
    "    \n",
    "    nn.Linear(20,action_space_size),\n",
    ")\n",
    "criterion = nn.MSELoss()\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.Adam(bc_walker.parameters(), lr = learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list = []\n",
    "test_loss = []\n",
    "batch_size = 256\n",
    "n_epoch = 100\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(bc_walker.parameters(), lr = learning_rate) \n",
    "for itr in range(n_epoch):\n",
    "    total_loss = 0\n",
    "    b=0\n",
    "    for batch in range (0,training_set.shape[0], batch_size):\n",
    "        data   = training_set  [batch : batch+batch_size , :state_space_size]\n",
    "        y      = training_set [batch : batch+batch_size, state_space_size:]\n",
    "        y_pred = bc_walker(data)\n",
    "        loss   = criterion(y_pred, y)\n",
    "        total_loss += loss.item() \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        b += 1\n",
    "    print(\"[EPOCH]: %i, [MSE LOSS]: %.6f\" % (itr+1, total_loss / b))\n",
    "    display.clear_output(wait=True)\n",
    "    loss_list.append(total_loss / b)\n",
    "    x = testing_set[:, :state_space_size]\n",
    "    y = testing_set[:,state_space_size:]\n",
    "    y_pred = bc_walker(x)\n",
    "    test_loss.append(criterion(y_pred, y).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot test loss\n",
    "# torch.save(bc_walker, \"bc_walker_n=2\") # uncomment to save the model \n",
    "plt.plot(test_loss, label=\"Testing Loss\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test inferred actions with real actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 87 # select any point to test the model\n",
    "print( bc_walker(testing_set[p, :state_space_size]) )\n",
    "print(testing_set[p, state_space_size:])\n",
    "criterion( bc_walker(testing_set[p, :state_space_size] ), testing_set[p, state_space_size:] ).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test BC model in Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## parameters ##################################\n",
    "n=2 # window size\n",
    "n_iterations = 5 # max number of interacting with environment\n",
    "n_ep = 1000 # number of epoches\n",
    "max_steps = 500 # max timesteps per epoch\n",
    "gamma = 1.0 # discount factor\n",
    "seeds = [684, 559, 629, 192, 835] # random seeds for testing\n",
    "################################## parameters ##################################\n",
    "\n",
    "seed_reward_mean = []\n",
    "seed_reward  = []\n",
    "for itr in range (n_iterations):\n",
    "   ################################## interact with env ##################################\n",
    "    G= []\n",
    "    G_mean = []\n",
    "    env.seed(int(seeds[itr]))\n",
    "    torch.manual_seed(int(seeds[itr]))\n",
    "    torch.cuda.manual_seed_all(int(seeds[itr]))\n",
    "\n",
    "    for ep in range (n_ep):\n",
    "        state = env.reset()\n",
    "        rewards = []\n",
    "        R=0\n",
    "        for t in range (max_steps):      \n",
    "            action = bc_walker(torch.tensor(state, dtype=torch.float))\n",
    "            action = np.clip(action.detach().numpy(), -1,1)\n",
    "            next_state , r, done, _   = env.step(action)\n",
    "            rewards.append(r)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        R = sum([rewards[i]*gamma**i for i in range (len(rewards))])\n",
    "        G.append(R)\n",
    "        G_mean.append(np.mean(G))\n",
    "        if ep % 1 ==0:\n",
    "            print(\"ep = {} , Mean Reward = {:.6f}\".format(ep, R))\n",
    "        display.clear_output(wait=True)\n",
    "    seed_reward.append(G)\n",
    "    seed_reward_mean.append(G_mean)\n",
    "    print(\"Itr = {} overall reward  = {:.6f} \".format(itr, np.mean(seed_reward_mean[-1])))\n",
    "    print(\"Interacting with environment finished\")\n",
    "env.close()\n",
    "# np.save(\"reward_mean_walker_bc1_expert_states={}\".format(new_data.shape[0]), seed_reward_mean) #uncomment to save reward over 5 random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_reward_mean_bc = np.array(seed_reward_mean)\n",
    "mean_bc  = np.mean(seed_reward_mean_bc,axis=0)\n",
    "std_bc  = np.std(seed_reward_mean_bc,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRPO_mean  = np.load(\"reward_mean_expert_walker_TRPO.npy\")\n",
    "mean_expert= np.mean(TRPO_mean,axis=0)\n",
    "std_expert = np.std(TRPO_mean,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_mean  = np.load(\"reward_mean_walker_random.npy\")\n",
    "mean_random= np.mean(random_mean,axis=0)\n",
    "std_random  = np.std(random_mean,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled performance\n",
    "def scaled (x, min_value, max_value):\n",
    "    return (x - min_value) / (max_value - min_value)\n",
    "\n",
    "bc_score  = scaled( mean_bc[-1] , mean_random[-1] , mean_expert[-1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare BC VS Expert VS Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(1000)\n",
    "\n",
    "plt.plot(x, mean_expert, \"-\", label=\"Expert\")\n",
    "plt.fill_between(x, mean_expert+std_expert, mean_expert-std_expert, alpha=0.2)\n",
    "\n",
    "plt.plot(x, mean_bc, \"-\", label=\"BC\")\n",
    "plt.fill_between(x, mean_bc + std_bc, mean_bc - std_bc, alpha=0.2)\n",
    "\n",
    "plt.plot(x, mean_random, \"-\", label=\"Random\")\n",
    "plt.fill_between(x, mean_random+std_random, mean_random-std_random, alpha=0.2)\n",
    "\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Mean Reward\")\n",
    "plt.title(\"Random VS Expert VS BC in Walker\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
